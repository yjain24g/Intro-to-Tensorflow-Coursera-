{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Natural Language Processing Week 4.ipynb","provenance":[],"authorship_tag":"ABX9TyN37qFFKidoMH653FBJzMVr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"qYifSFAKvFxz"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import regularizers\n","import tensorflow.keras.utils as ku \n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAN1sCq7vMAP"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n","    -O /tmp/sonnets.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0J3584gvOfN"},"source":["data = open('/tmp/sonnets.txt').read()\n","corpus = data.lower().split(\"\\n\")\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# create input sequences using list of tokens\n","input_sequences = []\n","for line in corpus:\n","\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n","\tfor i in range(1, len(token_list)):\n","\t\tn_gram_sequence = token_list[:i+1]\n","\t\tinput_sequences.append(n_gram_sequence)\n","\n","# pad sequences \n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# create predictors and label\n","predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n","\n","label = ku.to_categorical(label, num_classes=total_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iES5lR0JvRuq"},"source":["model = Sequential()\n","model.add(Embedding(total_words, 128, input_length=max_sequence_len - 1))\n","model.add(Bidirectional(LSTM(120, return_sequences=True)))\n","model.add(Dropout(0.2))\n","model.add(LSTM(96))\n","model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","model.add(Dense(total_words, activation='softmax'))\n","\n","\n","model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")\n","\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H4BMTOzMvUdr"},"source":["history = model.fit(predictors, label, epochs=100, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1bN2vyIvW7b"},"source":["import matplotlib.pyplot as plt\n","acc = history.history['acc']\n","loss = history.history['loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'b', label='Training accuracy')\n","plt.title('Training accuracy')\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'b', label='Training Loss')\n","plt.title('Training loss')\n","plt.legend()\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ymRQin5_vbFs"},"source":["seed_text = \"The true purpose of deep learning is making memes because\"\n","next_words = 100\n","  \n","for _ in range(next_words):\n","\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n","\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","\tpredicted = model.predict_classes(token_list, verbose=0)\n","\toutput_word = \"\"\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == predicted:\n","\t\t\toutput_word = word\n","\t\t\tbreak\n","\tseed_text += \" \" + output_word\n","\n","print(seed_text)"],"execution_count":null,"outputs":[]}]}
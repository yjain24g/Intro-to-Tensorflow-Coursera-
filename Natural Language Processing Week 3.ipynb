{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Natural Language Processing Week 3.ipynb","provenance":[],"authorship_tag":"ABX9TyPgrHQCp0MsVGMlp1pPchvX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"EgFycgfzvN7G"},"source":["import json\n","import tensorflow as tf\n","import csv\n","import random\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import regularizers\n","\n","\n","embedding_dim = 100\n","max_length = 16\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","training_size=160000\n","test_portion=.1\n","\n","corpus = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KO4upjLJvXJ-"},"source":["\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n","    -O /tmp/training_cleaned.csv\n","\n","num_sentences = 0\n","\n","with open(\"/tmp/training_cleaned.csv\") as csvfile:\n","    reader = csv.reader(csvfile, delimiter=',')\n","    for row in reader:\n","        list_item=[]\n","        list_item.append(row[5])\n","        this_label=row[0]\n","        if this_label=='0':\n","            list_item.append(0)\n","        else:\n","            list_item.append(1)\n","        num_sentences = num_sentences + 1\n","        corpus.append(list_item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TmO_swpQvgpf"},"source":["print(num_sentences)\n","print(len(corpus))\n","print(corpus[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x93rsH7WvkQc"},"source":["sentences=[]\n","labels=[]\n","random.shuffle(corpus)\n","for x in range(training_size):\n","    sentences.append(corpus[x][0])\n","    labels.append(corpus[x][1])\n","\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","\n","word_index = tokenizer.word_index\n","vocab_size=len(word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","split = int(test_portion * training_size)\n","\n","test_sequences = padded[0:split]\n","training_sequences = padded[split:training_size]\n","test_labels = labels[0:split]\n","training_labels = labels[split:training_size]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bjax-E7Xvk2K"},"source":["print(vocab_size)\n","print(word_index['i'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"baINSlq3vn-Q"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n","    -O /tmp/glove.6B.100d.txt\n","embeddings_index = {};\n","with open('/tmp/glove.6B.100d.txt') as f:\n","    for line in f:\n","        values = line.split();\n","        word = values[0];\n","        coefs = np.asarray(values[1:], dtype='float32');\n","        embeddings_index[word] = coefs;\n","\n","embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word);\n","    if embedding_vector is not None:\n","        embeddings_matrix[i] = embedding_vector;"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmoAqtnMwKit"},"source":["print(len(embeddings_matrix))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A46p1rfrwLQS"},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n","    tf.keras.layers.MaxPooling1D(pool_size=4),\n","    tf.keras.layers.LSTM(64),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()\n","\n","num_epochs = 50\n","history = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=2)\n","\n","print(\"Training Complete\")\n","#I went with Conv1D instead of GRU or LSTM just due to accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-rdVnAUwWbU"},"source":["import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['acc']\n","val_acc=history.history['val_acc']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r')\n","plt.plot(epochs, val_acc, 'b')\n","plt.title('Training and validation accuracy')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n","\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r')\n","plt.plot(epochs, val_loss, 'b')\n","plt.title('Training and validation loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend([\"Loss\", \"Validation Loss\"])\n","\n","plt.figure()\n"],"execution_count":null,"outputs":[]}]}